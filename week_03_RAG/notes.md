RAG notes
---------

What Are Embeddings?

Embeddings are a way to convert complex data into numbers â€” specifically, dense vectors â€” that a machine learning model can easily understand and work with.

Think of them as compressed meaning.

â¸»

ğŸ“ Why Are They Important?

Most ML models canâ€™t directly understand:
	â€¢	Text like â€œappleâ€
	â€¢	Audio like your voice
	â€¢	Products or movies

Embeddings translate those things into vectors (e.g., [0.13, -1.42, 3.01, ...]) that capture their meaning, features, and relationships.

---
ğŸ§  What is Cosine Similarity?

Cosine similarity is a way to measure how similar two vectors are by looking at the angle between them â€” not their size.

Imagine two arrows (vectors) starting from the same point:
- If they point in the same direction â†’ Very similar â†’ Cosine similarity = +1
- If they are completely unrelated (90Â°) â†’ Cosine similarity = 0
- If they point in opposite directions â†’ Very different â†’ Cosine similarity = -1

ğŸ¯ Analogy:
- "I love dogs" and "I like puppies" â†’ Similar vectors â†’ High cosine similarity
- "I love dogs" and "I crashed my car" â†’ Very different â†’ Low cosine similarity

ğŸ“ Why â€œCosineâ€?
It uses the cosine of the angle between vectors:
- 0Â° â†’ cos(0) = 1 â†’ same direction
- 90Â° â†’ cos(90) = 0 â†’ unrelated
- 180Â° â†’ cos(180) = -1 â†’ opposite direction

ğŸ¤– In LLMs and vector databases:
Cosine similarity is used to compare the query vector with document vectors and find the most semantically similar ones.

âœ… TL;DR:
Cosine similarity = How close in direction two data points are in vector space. It's essential in NLP, vector search, and semantic similarity tasks.
---

